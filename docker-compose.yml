version: '3.8'

services:

  # vllm_server:
  #   image: vllm/vllm-openai:latest
  #   runtime: nvidia
  #   environment:
  #     - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_API_KEY}
  #     - MODEL_NAME=${MODEL_NAME}
  #   volumes:
  #     - ~/.cache/huggingface:/root/.cache/huggingface
  #     - //c/Users/julie/Desktop/gcloudllm/CloudRun-LLM/model:/model
  #   ports:
  #     - "8000:8000"
  #   ipc: host
  #   command: >
  #     --model /model/qwen2.5-coder-7b-instruct-q3_k_m.gguf
  #     --gpu-memory-utilization 0.95
  #     --max-model-len 8192
  #     --kv-cache-dtype auto
  #     --max-num-seqs 128
  #   deploy:
  #     resources:
  #       reservations:
  #         devices:
  #           - driver: nvidia
  #             count: all
  #             capabilities: [gpu]

  
  myapp:
    build:
      context: .
      dockerfile: DockerfileLocal
    env_file:
      - .env
    volumes:
      - //c/Users/julie/Desktop/gcloudllm/CloudRun-LLM/model:/model
    ports:
      - "${PORT}:${PORT}"
    # depends_on:
    #   - vllm_server

volumes:
  model_data: