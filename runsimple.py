from fastapi import FastAPI, Request
from typing import List, Optional
import uvicorn
from fastapi.responses import StreamingResponse, JSONResponse

from contextlib import asynccontextmanager
from vllm.entrypoints.openai.serving_tokenization import (
    OpenAIServingTokenization)
from vllm.entrypoints.openai.serving_models import OpenAIServingModels, BaseModelPath
from vllm.entrypoints.openai.serving_engine import OpenAIServing
from vllm.entrypoints.openai.serving_chat import OpenAIServingChat
from vllm.engine.async_llm_engine import AsyncLLMEngine, AsyncEngineArgs
from vllm.entrypoints.openai.serving_completion import OpenAIServingCompletion
from vllm.entrypoints.utils import with_cancellation
from vllm.entrypoints.openai.protocol import (ChatCompletionRequest,
                                              ChatCompletionResponse,
                                              CompletionRequest,
                                              CompletionResponse, ErrorResponse)
import os

# DÃ©finition du modÃ¨le
MODEL_NAME =  os.getenv('MODEL_NAME') # "/model/Qwen2.5-Coder-1.5B-Instruct-Q8_0.gguf" # "Qwen/Qwen2.5-1.5B-Instruct"
print("MODEL_NAME ðŸš€", MODEL_NAME)

"""
    App from FastAPI
"""
@asynccontextmanager
async def lifespan(app: FastAPI):
    engine, openai_serving_chat = await init_app()
    app.state.engine = engine
    app.state.openai_serving_chat = openai_serving_chat
    yield

    await app.state.engine.close()



app = FastAPI(lifespan=lifespan)


"""
    Init App
"""
async def init_app():

    # Initialisation du moteur de maniÃ¨re asynchrone
    engine_args = AsyncEngineArgs(model=MODEL_NAME,
        quantization="awq", 
        dtype="auto",
        # cpu_offload_gb=10,
        max_model_len=16384,
        gpu_memory_utilization=0.95
    )
    engine =  AsyncLLMEngine.from_engine_args(engine_args)

    # Obtention de la configuration du modÃ¨le
    model_config = await engine.get_model_config()
    

    """
        engine_client : Une instance de EngineClient, qui est le moteur d'infÃ©rence asynchrone pour le modÃ¨le de langage.
        model_config : Un objet ModelConfig contenant la configuration du modÃ¨le.
        base_model_paths : Une liste de BaseModelPath, oÃ¹ chaque Ã©lÃ©ment contient le nom et le chemin du modÃ¨le de base. Dans ce cas, il n'y a qu'un seul modÃ¨le spÃ©cifiÃ© avec le nom et le chemin dÃ©finis par MODEL_NAME1.
        lora_modules : DÃ©fini Ã  None, ce paramÃ¨tre permet d'ajouter des modules LoRA (Low-Rank Adaptation) au modÃ¨le. LoRA est une technique d'adaptation efficace pour les grands modÃ¨les de langage2.
        prompt_adapters : Ã‰galement dÃ©fini Ã  None, ce paramÃ¨tre permet d'ajouter des adaptateurs de prompt au modÃ¨le. Les adaptateurs de prompt sont des mÃ©thodes d'apprentissage efficaces pour adapter les modÃ¨les de vision et de langage Ã  de nouvelles tÃ¢ches68.
    """

     # CrÃ©ation de OpenAIServingModels
    openai_serving_models = OpenAIServingModels(
        engine_client=engine,
        model_config=model_config,
        base_model_paths=[BaseModelPath(name=MODEL_NAME, model_path=MODEL_NAME)],
        lora_modules=None,  # Ajustez selon vos besoins
        prompt_adapters=None,  # Ajustez selon vos besoins
    )
    await openai_serving_models.init_static_loras()
    
    # CrÃ©ation de OpenAIServingChat
    app.state.openai_serving_chat = OpenAIServingChat(
        engine_client=engine,  # Le moteur d'infÃ©rence asynchrone pour  le modÃ¨le
        model_config=model_config, # La configuration du modÃ¨le
        models=openai_serving_models, # Instance de OpenAIServingModels contenant les informations sur les modÃ¨les disponibles
        response_role="assistant", # Le rÃ´le attribuÃ© aux rÃ©ponses gÃ©nÃ©rÃ©es par le modÃ¨le
        request_logger=None,   # Logger pour les requÃªtes, dÃ©sactivÃ© ici
        chat_template=None,  # Template de chat personnalisÃ©, non utilisÃ© ici
        chat_template_content_format="auto" 
    )

    app.state.openai_serving_tokenization = OpenAIServingTokenization(
        engine_client = engine,
        model_config=model_config,
        models=openai_serving_models,
        request_logger=None,
        chat_template=None,  # Template de chat personnalisÃ©, non utilisÃ© ici
        chat_template_content_format="auto" 
    )

    app.state.openai_serving_completion = OpenAIServingCompletion(
       engine_client = engine,
       model_config=model_config,
       models=openai_serving_models,
        request_logger=None,
        return_tokens_as_token_ids=False,
    )

    return engine, app.state.openai_serving_chat 



"""
    Handlers with state of app (Management of State)
"""
def chat(request: Request) -> Optional[OpenAIServingChat]:
    return request.app.state.openai_serving_chat

def tokenization(request: Request) -> OpenAIServingTokenization:
    return request.app.state.openai_serving_tokenization

def base(request: Request) -> OpenAIServing:
    return tokenization(request)

def completion(request: Request) -> Optional[OpenAIServingCompletion]:
    return request.app.state.openai_serving_completion


"""
   Routing for the API
"""

# Endpoint pour /v1/chat/completions (compatible OpenAI Chat API)
@app.post("/v1/chat/completions")
@with_cancellation
async def create_chat_completion(request: ChatCompletionRequest,
                                 raw_request: Request):
    handler = chat(raw_request)
    if handler is None:
        return base(raw_request).create_error_response(
            message="The model does not support Chat Completions API")

    generator = await handler.create_chat_completion(request, raw_request)

    if isinstance(generator, ErrorResponse):
        return JSONResponse(content=generator.model_dump(),
                            status_code=generator.code)

    elif isinstance(generator, ChatCompletionResponse):
        return JSONResponse(content=generator.model_dump())

    return StreamingResponse(content=generator, media_type="text/event-stream")


@app.post("/v1/completions")
@with_cancellation
async def create_completion(request: CompletionRequest, raw_request: Request):
    handler = completion(raw_request)
    if handler is None:
        return base(raw_request).create_error_response(
            message="The model does not support Completions API")

    generator = await handler.create_completion(request, raw_request)
    if isinstance(generator, ErrorResponse):
        return JSONResponse(content=generator.model_dump(),
                            status_code=generator.code)
    elif isinstance(generator, CompletionResponse):
        return JSONResponse(content=generator.model_dump())

    return StreamingResponse(content=generator, media_type="text/event-stream")




# Lancement du serveur avec Uvicorn
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8080)
